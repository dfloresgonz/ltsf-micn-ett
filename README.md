# ğŸ“Œ MICN â€“ Experimentos de Forecasting (ETT Dataset)

Este repositorio implementa y entrena el modelo **Multiâ€‘scale Isometric Convolution Network (MICN)** para pronÃ³stico de series de tiempo de horizonte largo (LTSF) usando PyTorch.  
Incluye integraciÃ³n con **MLflow** para el tracking de experimentos.

---

## âš™ï¸ InstalaciÃ³n

1. **Clonar el repositorio**

```bash
git clone https://github.com/tu_usuario/micn-experiments.git
cd micn-experiments
```

2. **Crear y activar un entorno virtual (recomendado)**

```bash
python3 -m venv venv
source venv/bin/activate   # macOS / Linux
# .\venv\Scripts\activate  # Windows
```

3. **Instalar dependencias**

```bash
pip install -r requirements.txt
```

> ğŸ“Œ AsegÃºrate de tener instalada la versiÃ³n de PyTorch con soporte para **MPS** en Mac (Metal Performance Shaders).  
> Para verificar:
> ```python
> import torch
> print(torch.backends.mps.is_available())
> ```

---

## ğŸš€ Uso de MLflow

Este proyecto utiliza **MLflow** para registrar parÃ¡metros, mÃ©tricas y artefactos.

### 1ï¸âƒ£ Instalar MLflow

```bash
pip install mlflow
```

### 2ï¸âƒ£ Levantar el servidor local de MLflow

Desde la raÃ­z del proyecto:

```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns
```

Esto levantarÃ¡ la interfaz web en [http://127.0.0.1:5000](http://127.0.0.1:5000)

> Puedes cambiar el puerto con `--port 8080` si lo necesitas:
> ```bash
> mlflow ui --port 8080
> ```

El cÃ³digo del trainer ya tiene configurado:

```python
mlflow.set_tracking_uri("http://127.0.0.1:5000")
```

AsÃ­ que los experimentos se loguearÃ¡n automÃ¡ticamente al servidor local.

---

## ğŸ§  Arquitectura del Modelo MICN

El modelo **MICN (Multi-scale Isometric Convolution Network)** es una arquitectura diseÃ±ada especÃ­ficamente para el pronÃ³stico de series temporales de horizonte largo (**LTSF**), donde los modelos tradicionales (como LSTM o Transformer) suelen degradar su rendimiento debido a la dificultad de capturar patrones globales y multiescala.

MICN propone una arquitectura basada enteramente en **convoluciones isomÃ©tricas** para extraer caracterÃ­sticas de manera eficiente a diferentes escalas temporales.

### âœ¨ MotivaciÃ³n del diseÃ±o

- Los mÃ©todos autoregresivos propagan errores paso a paso en LTSF.
- Los modelos recurrentes como LSTM son difÃ­ciles de paralelizar y tienen problemas con dependencias largas.
- Los Transformers son potentes, pero costosos y no necesariamente eficaces para series altamente estructuradas y estacionarias.

### ğŸ§© Componentes del MICN

```mermaid
flowchart LR
    A[input: Xâ‚œ] --> B[Multi-scale Hybrid Decomposition]
    B --> C[Xâ‚›]
    B --> D[Xâ‚œ]

    C --> E[Embedding]
    E --> F[MIC Block]
    F --> G[Yâ‚›]

    D --> H[Regression]
    H --> I[Yâ‚œ]

    G --> J[Sum âŠ•]
    I --> J
    J --> K[Y_pred]
```

### ğŸ” DescripciÃ³n de cada componente:

- **Input Series X (t):**  
  Serie temporal univariada u multivariada (en este caso la variable "OT") dividida en ventanas de longitud fija (`input_len`).  

- **Embedding Layer:**  
  Aplica una capa lineal o convolucional que proyecta la entrada a un espacio latente de dimensiÃ³n `d_model`, comÃºn a todas las escalas.

- **Multi-scale Isometric Convolution Blocks:**  
  Cada bloque aplica convoluciones con diferentes `kernel sizes` (por ejemplo: 12, 24, 48) pero con **padding isomÃ©trico**, lo que garantiza que la salida conserve la misma longitud temporal.  
  Esto permite capturar **patrones locales y globales** sin pÃ©rdida de alineaciÃ³n temporal.  
  AdemÃ¡s, evita el uso de pooling o dilated convolutions, lo cual reduce complejidad y artefactos.

- **Feature Aggregation:**  
  Las salidas de cada bloque de escala se combinan (por suma, concatenaciÃ³n o atenciÃ³n) para formar una representaciÃ³n rica multiescala.

- **Decoder Layer:**  
  Una o mÃ¡s capas lineales que proyectan la representaciÃ³n agregada hacia el horizonte de salida `output_len`.

- **Forecast Y (t+h):**  
  Salida final: predicciÃ³n multihorizonte de la serie a futuro.

### ğŸ“ˆ Ventajas de MICN

- Completamente convolucional â†’ altamente paralelizable.
- Multiâ€‘scale design â†’ captura patrones en diferentes frecuencias.
- Isometric design â†’ mantiene alineaciÃ³n temporal sin distorsiones.
- Menor cantidad de parÃ¡metros que arquitecturas tipo Transformer.
- Excelente rendimiento en datasets ETTh/ETTm con bajo error (MAE/MSE).

---

## ğŸ“‚ Arquitectura del Repositorio

```
micn-experiments/
â”œâ”€ experiments/
â”‚  â”œâ”€ trainer.py           # Entrenamiento y evaluaciÃ³n del modelo
â”‚
â”œâ”€ micn/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ MICNModel.py         # ImplementaciÃ³n del modelo MICN
â”‚
â”œâ”€ utils/
â”‚  â”œâ”€ windowing.py         # Funciones para crear ventanas y splits
â”‚  â”œâ”€ metrics.py           # MÃ©tricas MSE, MAE
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ ETTh1.csv            # Dataset (ejemplo)
â”‚  â”œâ”€ ETTh2.csv
â”‚  â”œâ”€ ...
â”‚
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ mlruns/                 # Carpeta generada por MLflow con los experimentos
```

---

## ğŸ“ EjecuciÃ³n del Experimento

```bash
python experiments/trainer.py
```

En tu script puedes definir parÃ¡metros como:

```python
params = {
  "dataset": "ETTh1",
  "input_len": 96,
  "output_len": 24,
  "d_model": 64,
  "n_layers": 2,
  "scales": (12,24,48),
  "batch_size": 32,
  "epochs": 10,
  "learning_rate": 0.001
}
```
